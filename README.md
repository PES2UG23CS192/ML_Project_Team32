# ML_Project_Team32
# Measuring and Incorporating Correlations in Generative Adversarial Networks

This project investigates techniques to improve **Generative Adversarial Network (GAN)** performance by explicitly modeling **feature correlations** during the image generation process.  
The objective is to explore **correlation-based loss functions** and **conditioning mechanisms** to enhance the quality and diversity of generated samples.

---

## ğŸ§  Project Overview

Traditional GANs often struggle to capture inter-feature dependencies effectively.  
This project focuses on incorporating correlation information into the training process to stabilize GAN training and produce more realistic outputs.

Key ideas explored:
- Modeling correlations between latent and generated features.  
- Using correlation-based regularization or loss terms.  
- Evaluating generated images using **FID (FrÃ©chet Inception Distance)** and **Inception Score (IS)**.  

---

## ğŸ“Š Dataset

You can use open or synthetic datasets such as:
- **CIFAR-10**
- **CelebA**
- Custom or synthetic tabular datasets  

Datasets can be loaded using PyTorchâ€™s `torchvision.datasets` or from public repositories.

---

## âš™ï¸ Implementation Details

The project was implemented in **Python** using **Google Colab** for GPU acceleration.  

### Major Libraries Used:
- `torch`, `torchvision`, `torch.nn`
- `numpy`, `matplotlib`
- `torch-fidelity` for FID/IS evaluation

---

## ğŸš€ How to Run on Google Colab

1. Open [Google Colab](https://colab.research.google.com/).  
2. Upload the notebook: **ML_Project_192_185.ipynb**.  
3. Enable **GPU Runtime**:  
   ```
   Runtime â†’ Change runtime type â†’ Hardware accelerator â†’ GPU
   ```
4. Run all cells sequentially.  
5. The notebook will:
   - Train a GAN on your chosen dataset  
   - Compute **FID** and/or **Inception Score** for evaluation  

---

## ğŸ“ˆ Evaluation Metrics

- **Inception Score (IS):** Measures sample diversity and quality.  
- **FrÃ©chet Inception Distance (FID):** Measures distance between real and generated distributions.  

Lower FID and higher IS indicate better model performance.

---

## ğŸ§© Possible Extensions

- Experiment with **correlation-based discriminators**.  
- Introduce **attention mechanisms** to capture feature dependencies.  
- Apply to **conditional GANs (cGANs)** or **StyleGAN architectures**.  

---

## ğŸ“š References

- Goodfellow et al., *â€œGenerative Adversarial Netsâ€*, NeurIPS 2014.  
- Heusel et al., *â€œGANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibriumâ€*, NeurIPS 2017.  
- Radford et al., *â€œUnsupervised Representation Learning with Deep Convolutional Generative Adversarial Networksâ€*, 2016.

---

## ğŸ‘©â€ğŸ’» Authors

- **Drishti Golchha (PES2UG23CS185)**  
- Under the guidance of **[Your Instructorâ€™s Name or Course Title]**
